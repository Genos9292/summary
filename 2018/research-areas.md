
Yoshua Bengio:

<p class="ui_qtext_para">Every researcher has their opinion on this, which is a good thing. Here are some I see:</p><ul><li>Unsupervised learning that would really kick ass</li><ul><li>generative models that generate crisp images and sounds over a wide set of variations covering natural images and sounds</li><li>semi-supervised learning that makes a difference even when the labeled dataset is not tiny</li><li>learning a two-way transformation of the data into a space where variables are disentangled (or mostly independent)</li><li>bringing (iterative) inference back in deep learning to handle non-factorial posteriors over the latent variables</li></ul><li>Introducing more reasoning abilities in our models</li><li>Natural language understanding and knowledge representation on a large scale</li><li>Models of really long-term dependencies in sequential data and having learners that discover a hierarchical representation at multiple time scales </li><li>Understand better (and fix) the optimization issues that sometimes arise (for example in unsupervised learning or recurrent nets with long-term dependencies)</li><li>Train models that incorporate planning (playing out what-if scenarios, maybe stochastically via a generative component) as part of the learning procedure (and also to actually take decisions)</li><li>Scaling up reinforcement learning to large action spaces</li><li>Maximum likelihood has known deficiencies (such as a mismatch between training and test conditions) and we need to go around them (maybe ditching maximum likelihood altogether)</li><li>Bridging the gap between deep learning and biology</li><li>Ramping up our theoretical understanding of deep learning (optimization issues being one aspect, but expressive / statistical aspects being also in need of more theory)</li><li>Building specialized hardware that will allow us not just to build consumer products from models trained offline, but maybe more importantly from a scientific point of view, to train much larger models which capture much more knowledge, so as to open the way towards human-level AI</li><li>Many applications which are under-explored, and in particular I would like to see much more work in the area of health (with some specific issues like missing values and being to exploit data from small studies via transfer learning)</li></ul></span></div></div>
