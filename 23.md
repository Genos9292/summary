- [Bioformer: an efficient transformer language model for biomedical text mining](https://arxiv.org/ftp/arxiv/papers/2302/2302.01588.pdf)
- [Top-Down Beats Bottom-Up in 3D Instance Segmentation](https://arxiv.org/pdf/2302.02871v1.pdf)
- [EfficientRep:An Efficient Repvgg-style ConvNets with Hardware-aware Neural Network Design](https://arxiv.org/pdf/2302.00386v1.pdf)
- [Cut and Learn for Unsupervised Object Detection and Instance Segmentation](https://arxiv.org/pdf/2301.11320v1.pdf)
- [The Wisdom of Hindsight Makes Language Models Better Instruction Followers](https://arxiv.org/pdf/2302.05206v1.pdf)
- [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/pdf/2302.00923v2.pdf)
- [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/pdf/2211.10438v3.pdf)
- [Towards Robust Blind Face Restoration with Codebook Lookup Transformer](https://arxiv.org/pdf/2206.11253v2.pdf)
- [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/pdf/2301.12597v1.pdf)
- [DAMO-YOLO : A Report on Real-Time Object Detection Design](https://arxiv.org/pdf/2211.15444v2.pdf)
- [Instant Neural Graphics Primitives with a Multiresolution Hash Encoding](https://arxiv.org/pdf/2201.05989v2.pdf)
- [A New Outlier Removal Strategy Based on Reliability of Correspondence Graph for Fast Point Cloud Registration](https://arxiv.org/pdf/2205.07404v1.pdf)
- [Reversible Vision Transformers](https://arxiv.org/pdf/2302.04869v1.pdf)
- [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities]()
- [Fine-Tuning Language Models from Human Preferences]()
- [Dual PatchNorm]()
- [Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling]()
- []()
- []()
- []()
