
**Model**
------------------------------
**[14]** Improving neural networks by preventing co-adaptation of feature detectors. [[pdf]](https://arxiv.org/pdf/1207.0580.pdf) (Dropout)

[[summary]]  [revisit-notes] [[code](https://github.com/mdenil/dropout)] [[code](https://github.com/dnouri/cuda-convnet)] [[PPT](http://www.ke.tu-darmstadt.de/lehre/archiv/ws-13-14/seminarML/slides/folien13_Laux.pdf)]

**[15]** Dropout: a simple way to prevent neural networks from overfitting [[pdf]](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)

[[summary]]  [revisit-notes] [[code](https://github.com/yaringal/ConcreteDropout)] [[code](https://github.com/Philip-Bachman/NN-Dropout)] [[PPT](https://github.com/gopala-kr/summary/blob/master/summaries/Week-2/Lecture_04_Supervised_Pretraining.pptx)]

**[16]** Batch normalization: Accelerating deep network training by reducing internal covariate shift(2015). [[pdf]](http://arxiv.org/pdf/1502.03167) (An outstanding Work in 2015)

[[summary]]  [revisit-notes] [[code](https://github.com/ChenglongChen/batch_normalization)] [[code](https://github.com/shuuki4/Batch-Normalization)] [[code](https://github.com/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers)] [[code](https://github.com/ChenglongChen/batch_normalization)] [[PPT](http://people.ee.duke.edu/~lcarin/Zhao12.17.2015.pdf)]

**[17]** Layer normalization (2016). [[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) (Update of Batch Normalization)

[[summary]]  [revisit-notes] [[code](https://github.com/ryankiros/layer-norm)] [[code](https://github.com/carlthome/tensorflow-convlstm-cell)]  [[code](https://github.com/pbhatia243/tf-layer-norm)] [[code](https://github.com/MycChiu/fast-LayerNorm-TF)] [PPT]

**[18]** Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 orâˆ’1." [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) (New Model,Fast)

[[summary]]  [revisit-notes] [[code](https://github.com/codekansas/tinier-nn)] [[code](https://github.com/MatthieuCourbariaux/BinaryNet)] [[code](https://github.com/TianweiXing/BNN)] [[code](https://github.com/cornell-zhang/bnn-fpga)] [[PPT](http://web.eng.tau.ac.il/deep_learn/wp-content/uploads/2017/03/Binary-Deep-Learning.pdf)]

**[19]** Decoupled neural interfaces using synthetic gradients. [[pdf]](https://arxiv.org/pdf/1608.05343) (Innovation of Training Method,Amazing Work)

[[summary]]  [revisit-notes] [[code](https://github.com/andrewliao11/dni.pytorch)] [[code](https://github.com/vyraun/DNI-tensorflow)] [[PPT](https://www.slideshare.net/Eniod/019-20160907-decoupled-neural-interfaces-using-synthetic-gradients)]

**[20]** Net2net: Accelerating learning via knowledge transfer.(2015). [[pdf]](https://arxiv.org/abs/1511.05641) (Modify previously trained network to reduce training epochs)

[[summary]]  [revisit-notes] [[code](https://github.com/soumith/net2net.torch)] [[code](https://github.com/DanielSlater/Net2Net)] [[code](https://github.com/paengs/Net2Net)] [[code](https://github.com/erogol/Net2Net)] [PPT]

**[21]** Network Morphism. [[pdf]](https://arxiv.org/abs/1603.01670) (Modify previously trained network to reduce training epochs)

[[summary]]  [revisit-notes] [code] [PPT]


**Optimization**
---------------------------------------
**[22]** On the importance of initialization and momentum in deep learning. [[pdf]](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf) (Momentum optimizer)

[[summary]]  [revisit-notes] [code] [PPT] 

**[23]** Adam: A method for stochastic optimization. [[pdf]](http://arxiv.org/pdf/1412.6980) (Maybe used most often currently)

[[summary]]  [revisit-notes] [code] [[PPT](https://moodle2.cs.huji.ac.il/nu15/pluginfile.php/316969/mod_resource/content/1/adam_pres.pdf)] [[PPT](https://github.com/gopala-kr/summary/blob/master/summaries/Week-2/Adam_slides.pdf)]

**[24]** Learning to learn by gradient descent by gradient descent(2016). [[pdf]](https://arxiv.org/pdf/1606.04474) (Neural Optimizer,Amazing Work)

[[summary]]  [revisit-notes] [code] [PPT]

**[25]** Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. [[pdf]](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf) (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)

[[summary]]  [revisit-notes] [code] [[PPT](http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf)] [[PPT](https://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf)]

**[26]** SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size(2016). [[pdf]](http://arxiv.org/pdf/1602.07360) (Also a new direction to optimize NN,DeePhi Tech Startup)

[[summary]]  [revisit-notes] [code] [[PPT](http://statsmaths.github.io/stat665/lectures/lec19/lecture19.pdf)]
