
**Model**

**[14]** Improving neural networks by preventing co-adaptation of feature detectors. [[pdf]](https://arxiv.org/pdf/1207.0580.pdf) (Dropout)

[[summary]]  [revisit-notes] [code] [[PPT](http://www.ke.tu-darmstadt.de/lehre/archiv/ws-13-14/seminarML/slides/folien13_Laux.pdf)]

**[15]** Dropout: a simple way to prevent neural networks from overfitting [[pdf]](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)

[[summary]]  [revisit-notes] [code] [[PPT](https://github.com/gopala-kr/summary/blob/master/summaries/Week-2/Lecture_04_Supervised_Pretraining.pptx)]

**[16]** Batch normalization: Accelerating deep network training by reducing internal covariate shift(2015). [[pdf]](http://arxiv.org/pdf/1502.03167) (An outstanding Work in 2015)

[[summary]]  [revisit-notes] [code] [[PPT](http://people.ee.duke.edu/~lcarin/Zhao12.17.2015.pdf)]

**[17]** Layer normalization (2016). [[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) (Update of Batch Normalization)

[[summary]]  [revisit-notes] [code] [PPT]

**[18]** Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 orâˆ’1." [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) (New Model,Fast)

[[summary]]  [revisit-notes] [[code](https://github.com/codekansas/tinier-nn)] [PPT]

**[19]** Decoupled neural interfaces using synthetic gradients. [[pdf]](https://arxiv.org/pdf/1608.05343) (Innovation of Training Method,Amazing Work)

[[summary]]  [revisit-notes] [code] [PPT]

**[20]** Net2net: Accelerating learning via knowledge transfer.(2015). [[pdf]](https://arxiv.org/abs/1511.05641) (Modify previously trained network to reduce training epochs)

[[summary]]  [revisit-notes] [code] [PPT]

**[21]** Network Morphism. [[pdf]](https://arxiv.org/abs/1603.01670) (Modify previously trained network to reduce training epochs)

[[summary]]  [revisit-notes] [code] [PPT]
