
**Model**
------------------------------
**[14]** Improving neural networks by preventing co-adaptation of feature detectors. [[pdf]](https://arxiv.org/pdf/1207.0580.pdf) (Dropout)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/mdenil/dropout)] [[code](https://github.com/dnouri/cuda-convnet)] [[PPT](http://www.ke.tu-darmstadt.de/lehre/archiv/ws-13-14/seminarML/slides/folien13_Laux.pdf)]

**[15]** Dropout: a simple way to prevent neural networks from overfitting [[pdf]](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/yaringal/ConcreteDropout)] [[code](https://github.com/Philip-Bachman/NN-Dropout)] [[PPT](https://github.com/gopala-kr/summary/blob/master/summaries/Week-2/Lecture_04_Supervised_Pretraining.pptx)]

**[16]** Batch normalization: Accelerating deep network training by reducing internal covariate shift(2015). [[pdf]](http://arxiv.org/pdf/1502.03167) (An outstanding Work in 2015)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/ChenglongChen/batch_normalization)] [[code](https://github.com/shuuki4/Batch-Normalization)] [[code](https://github.com/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers)] [[code](https://github.com/ChenglongChen/batch_normalization)] [[PPT](http://people.ee.duke.edu/~lcarin/Zhao12.17.2015.pdf)]

**[17]** Layer normalization (2016). [[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) (Update of Batch Normalization)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/ryankiros/layer-norm)] [[code](https://github.com/carlthome/tensorflow-convlstm-cell)]  [[code](https://github.com/pbhatia243/tf-layer-norm)] [[code](https://github.com/MycChiu/fast-LayerNorm-TF)] [PPT]

**[18]** Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1." [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) (New Model,Fast)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/codekansas/tinier-nn)] [[code](https://github.com/MatthieuCourbariaux/BinaryNet)] [[code](https://github.com/TianweiXing/BNN)] [[code](https://github.com/cornell-zhang/bnn-fpga)] [[PPT](http://web.eng.tau.ac.il/deep_learn/wp-content/uploads/2017/03/Binary-Deep-Learning.pdf)]

**[19]** Decoupled neural interfaces using synthetic gradients. [[pdf]](https://arxiv.org/pdf/1608.05343) (Innovation of Training Method,Amazing Work)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/andrewliao11/dni.pytorch)] [[code](https://github.com/vyraun/DNI-tensorflow)] [[PPT](https://www.slideshare.net/Eniod/019-20160907-decoupled-neural-interfaces-using-synthetic-gradients)]

**[20]** Net2net: Accelerating learning via knowledge transfer.(2015). [[pdf]](https://arxiv.org/abs/1511.05641) (Modify previously trained network to reduce training epochs)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/soumith/net2net.torch)] [[code](https://github.com/DanielSlater/Net2Net)] [[code](https://github.com/paengs/Net2Net)] [[code](https://github.com/erogol/Net2Net)] [PPT]

**[21]** Network Morphism. [[pdf]](https://arxiv.org/abs/1603.01670) (Modify previously trained network to reduce training epochs)

✔ [[summary]]  [revisit-notes] [code] [PPT]


**Optimization**
---------------------------------------
**[22]** On the importance of initialization and momentum in deep learning. [[pdf]](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf) (Momentum optimizer)

✔ [[summary]]  [revisit-notes] [code] [PPT] 

**[23]** Adam: A method for stochastic optimization. [[pdf]](http://arxiv.org/pdf/1412.6980) (Maybe used most often currently)

✔ [[summary]]  [revisit-notes] [code] [[PPT](https://moodle2.cs.huji.ac.il/nu15/pluginfile.php/316969/mod_resource/content/1/adam_pres.pdf)] [[PPT](https://github.com/gopala-kr/summary/blob/master/summaries/Week-2/Adam_slides.pdf)]

**[24]** Learning to learn by gradient descent by gradient descent(2016). [[pdf]](https://arxiv.org/pdf/1606.04474) (Neural Optimizer,Amazing Work)

✔ [[summary]]  [revisit-notes] [code] [PPT]

**[25]** Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. [[pdf]](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf) (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)

✔ [[summary]]  [revisit-notes] [code] [[PPT](http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf)] [[PPT](https://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf)]

**[26]** SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size(2016). [[pdf]](http://arxiv.org/pdf/1602.07360) (Also a new direction to optimize NN,DeePhi Tech Startup)

✔ [[summary]]  [revisit-notes] [code] [[PPT](http://statsmaths.github.io/stat665/lectures/lec19/lecture19.pdf)]


**Unsupervised Learning / Deep Generative Model**
------------------------------
**[27]** Building high-level features using large scale unsupervised learning. [[arXiv](https://arxiv.org/abs/1112.6209)] (Milestone, Andrew Ng, Google Brain Project, Cat) 

✔ [[summary]]  [revisit-notes] [code] [[PPT](https://www.robots.ox.ac.uk/~vgg/rg/slides/ali_eslami__vgg_rg_slides.pdf)]

**[28]** Auto-encoding variational bayes.(2013). [[arXiv](https://arxiv.org/abs/1312.6114)] (VAE)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/peiyunh/mat-vae)] [[code](https://github.com/dillonalaird/VAE)] [[PPT](http://dpkingma.com/wordpress/wp-content/uploads/2014/05/2014-03_talk_iclr.pdf)] [[PPT](http://www.mlsalt.eng.cam.ac.uk/foswiki/pub/Main/ClassOf2017/D423C_poster.pdf)] [[PPT](http://people.ee.duke.edu/~lcarin/DEC9.26.2014.pdf)]

**[29]** Generative adversarial nets.2014. [[arXiv](https://arxiv.org/abs/1406.2661)] (GAN,super cool idea)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/junyanz/CycleGAN)] [[code](https://github.com/ckmarkoh/GAN-tensorflow)] [[PPT](https://www.slideshare.net/ssuser77ee21/generative-adversarial-networks-70896091)] [[PPT](https://www.slideshare.net/ckmarkohchang/generative-adversarial-networks)] [[PPT](http://dl.ee.cuhk.edu.hk/slides/gan.pdf)] [[PPT](http://www.cs.toronto.edu/~dtarlow/pos14/talks/goodfellow.pdf)] [[PPT](http://pages.cs.wisc.edu/~dpage/cs760/GANs.pdf)] [[PPT](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf)] [[PPT](http://slazebni.cs.illinois.edu/spring17/lec11_gan.pdf)]

**[30]** Unsupervised representation learning with deep convolutional generative adversarial networks.(2015). [[arXiv](https://arxiv.org/abs/1511.06434)] (DCGAN)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/Newmu/dcgan_code)] [[code](https://github.com/soumith/dcgan.torch)] [[code](https://github.com/mattya/chainer-DCGAN)] [[code](https://github.com/jacobgil/keras-dcgan)] [[code](https://github.com/YadiraF/GAN)] [[code](https://github.com/carpedm20/DCGAN-tensorflow)] [[PPT](http://aliensunmin.github.io/project/accv16tutorial/media/generative.pdf)]

**[31]** DRAW: A recurrent neural network for image generation.(2015). [[arXiv](https://arxiv.org/abs/1502.04623)] (VAE with attention, outstanding work)

✔ [[summary]]  [revisit-notes] [[code](https://github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW)] [[code](https://github.com/ericjang/draw)] [[code](https://github.com/jbornschein/draw)] [[code](https://github.com/vivanov879/draw)] [[code]()] [[PPT](http://people.ee.duke.edu/~lcarin/Zhe10.2.2015.pdf)] [[PPT](http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf)]

**[32]** Pixel recurrent neural networks.(2016). [[arXiv](https://arxiv.org/abs/1601.06759)] (PixelRNN)

✔ [[summary](https://gist.github.com/shagunsodhani/e741ebd5ba0e0fc0f49d7836e30891a7)] [[summay](https://github.com/sunshineatnoon/Paper-Collection/blob/master/pixel-rnn.md)] [revisit-notes] [[code](https://github.com/tensorflow/magenta/blob/master/magenta/reviews/pixelrnn.md)] [[code](https://github.com/carpedm20/pixel-rnn-tensorflow)] [[code](https://github.com/igul222/pixel_rnn)]  [[code]()] [[PPT](https://www.slideshare.net/neouyghur/pixel-recurrent-neural-networks-73970786)] [[PPT](https://github.com/gopala-kr/summary/blob/master/summaries/Week-2/lec10new.ppt)] [[PPT](http://imatge-upc.github.io/telecombcn-2016-dlcv/slides/D2L6-recurrent.pdf)] [[PPT](https://github.com/sunshineatnoon/Paper-Collection/blob/master/pixel-rnn.md)]

**[33]** Conditional image generation with PixelCNN decoders.(2016). [[arXiv](https://arxiv.org/abs/1606.05328)] (PixelCNN)

✔ [[summary](https://github.com/aleju/papers/blob/master/neural-nets/Conditional_Image_Generation_with_PixelCNN_Decoders.md)] [[summary](https://gist.github.com/shagunsodhani/3cc7066ce7de051d769908b8fab11990)]  [revisit-notes] [[code](https://github.com/anantzoid/Conditional-PixelCNN-decoder)] [[code](https://github.com/openai/pixel-cnn)] [[code](https://github.com/carpedm20/pixel-rnn-tensorflow)] [[code](https://github.com/kundan2510/pixelCNN)] [[code](https://github.com/dritchie/pixelCNN)] [[PPT](https://www.slideshare.net/suga93/conditional-image-generation-with-pixelcnn-decoders)] [[PPT](http://slazebni.cs.illinois.edu/spring17/lec13_advanced.pdf)]



**RNN / Sequence-to-Sequence Model**
-----------------------------------------

**[34]** Generating sequences with recurrent neural networks.(2013) [[arXiv]](http://arxiv.org/pdf/1308.0850) LSTM, very nice generating result, show the power of RNN)

✔ [[summary](https://github.com/tensorflow/magenta/blob/master/magenta/reviews/summary_generation_sequences.md)]  [revisit-notes] [[code](https://github.com/snowkylin/rnn-handwriting-generation)] [[PPT](https://www.slideshare.net/AndrePemmelaar/deep-lst-msandrnnsjulia)]

**[35]** Learning phrase representations using RNN encoder-decoder for statistical machine translation.(2014) [[arXiv]](http://arxiv.org/pdf/1406.1078) (First Seq-to-Seq Paper)

✔ [[summary](https://gist.github.com/shagunsodhani/9dccec626e68e495fd4577ecdca36b7b)]  [[summary](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/learning-phrase-representations.md)][revisit-notes] [code] [[PPT](https://www.slideshare.net/yutakikuchi927/learning-phrase-representations-using-rnn-encoderdecoder-for-statistical-machine-translation)]

**[36]** Sequence to sequence learning with neural networks.(2014) [[pdf]](http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf) (Outstanding Work)

✔ [[summary]()]  [revisit-notes] [[code](https://github.com/farizrahman4u/seq2seq)] [[code](https://github.com/pytorch/tutorials/blob/master/intermediate_source/seq2seq_translation_tutorial.py)] [[code](https://github.com/ichuang/tflearn_seq2seq)] [[code](https://github.com/harvardnlp/seq2seq-attn)] [[code](https://github.com/ma2rten/seq2seq)] [[code](https://github.com/JayParks/tf-seq2seq)] [[code](https://github.com/yoosan/mxnet-seq2seq)] [[code](https://github.com/fchollet/keras/issues/694)] [[code](https://gist.github.com/rouseguy/1122811f2375064d009dac797d59bae9)] [[PPT](https://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi4quPtivvWAhXMRY8KHSLVAxgQFggqMAA&url=https%3A%2F%2Fcomputing.ece.vt.edu%2F~f15ece6504%2Fslides%2FL17_Sequence.pptx&usg=AOvVaw2-5t4RNS-meOmSqdYxuqSS)] [[PPT](https://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=9&cad=rja&uact=8&ved=0ahUKEwi4quPtivvWAhXMRY8KHSLVAxgQFghZMAg&url=http%3A%2F%2Fdialog-systems-class.org%2Fslides%2Fseq2seq-lec-6.ppt&usg=AOvVaw3OSpZDHjJTdgDRtCh9LUK4)]  [[PPT](http://www.phontron.com/slides/neubig14taiwa11.pdf)] [[PPT](http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ilya_LSTMs_for_Translation.pdf)]

**[37]** Neural Machine Translation by Jointly Learning to Align and Translate. [[arXiv]](https://arxiv.org/pdf/1409.0473v7.pdf) 

✔ [[summary]]  [revisit-notes] [[code](https://github.com/tensorflow/nmt)] [[PPT](https://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjK-N6ajfvWAhUJq48KHbgrBuMQFggwMAE&url=http%3A%2F%2Fwww.statmt.org%2Fwmt15%2Fslides%2Finvited-talk.pptx&usg=AOvVaw18s856F0j82-tSTnpzA4gB)] [[PPT](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:bahdanau-iclr2015.pdf)]

**[38]** A neural conversational model.(2015) [[arXiv]](http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf))(Seq-to-Seq on Chatbot)

✔ [[summary](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/neural-conversational-model.md)]  [revisit-notes] [[code](https://github.com/macournoyer/neuralconvo)] [[code](https://github.com/Conchylicultor/DeepQA)] [[code](https://github.com/pbhatia243/Neural_Conversation_Models)] [[code](https://github.com/nicolas-ivanov/seq2seq_chatbot_links)] [[code](https://github.com/mckinziebrandon/DeepChatModels)] [[code](https://github.com/jiweil/Neural-Dialogue-Generation)] [[PPT](http://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC2523/conversation_berkay.pdf)] 
