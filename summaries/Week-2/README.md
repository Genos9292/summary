
**Model**

**[14]** Improving neural networks by preventing co-adaptation of feature detectors. [[pdf]](https://arxiv.org/pdf/1207.0580.pdf) (Dropout)

**[15]** Dropout: a simple way to prevent neural networks from overfitting [[pdf]](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)

**[16]** Batch normalization: Accelerating deep network training by reducing internal covariate shift(2015). [[pdf]](http://arxiv.org/pdf/1502.03167) (An outstanding Work in 2015)

**[17]** Layer normalization (2016). [[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) (Update of Batch Normalization)

**[18]** Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 orâˆ’1." [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) (New Model,Fast)

**[19]** Decoupled neural interfaces using synthetic gradients. [[pdf]](https://arxiv.org/pdf/1608.05343) (Innovation of Training Method,Amazing Work)

**[20]** Net2net: Accelerating learning via knowledge transfer.(2015). [[pdf]](https://arxiv.org/abs/1511.05641) (Modify previously trained network to reduce training epochs)

**[21]** Network Morphism. [[pdf]](https://arxiv.org/abs/1603.01670) (Modify previously trained network to reduce training epochs)
