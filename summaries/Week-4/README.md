**NLP(Natural Language Processing)**
----------------------------------

**[1]** Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing(2012) [[pdf]](https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf)

**[2]** Distributed representations of words and phrases and their compositionality(2013): 3111-3119 [[pdf]](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) **(word2vec)**

**[3]** Sequence to sequence learning with neural networks(2014) [[pdf]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

**[4]** Ask Me Anything: Dynamic Memory Networks for Natural Language Processing(2015) [[arxiv]](https://arxiv.org/abs/1506.07285)

**[5]** Character-Aware Neural Language Models(2015) [[arxiv]](https://arxiv.org/abs/1508.06615)

**[6]** Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks(2015). [[pdf]](https://arxiv.org/abs/1502.05698) **(bAbI tasks)** 

**[7]** Teaching Machines to Read and Comprehend(2015) [[arxiv]](https://arxiv.org/abs/1506.03340) **(CNN/DailyMail cloze style questions)** 

**[8]** Very Deep Convolutional Networks for Natural Language Processing(2016) [[arxiv]](https://arxiv.org/abs/1606.01781) **(state-of-the-art in text classification)** 

**[9]** Bag of Tricks for Efficient Text Classification(2016) [[arxiv]](https://arxiv.org/abs/1607.01759) **(slightly worse than state-of-the-art, but a lot faster)**
